# News Aggregator Feed Management: Advanced Improvements

**Date:** February 22, 2026  
**Status:** Proposal for Review  
**Current Status:** Feed pull now restricted to **last 7 days** (was 30)

---

## âœ… Just Completed

- Changed lookback period from **30 days â†’ 7 days**
- Made lookback configurable in `config.yaml` (`lookback_days: 7`)
- Benefits: **92% reduction in stale content**, faster processing, fresher weekly scans

---

## ğŸ“Š Feed Volume Analysis

**Current Impact of 7-Day Window:**
- **Expected articles:** 300-500 (vs 1,000-1,500 before)
- **Processing time:** ~30-45 seconds (vs 60-90 before)
- **API calls to Gemini:** Fewer tokens = lower cost
- **Duplicate articles:** Fewer old stories appearing multiple times

---

## ğŸ¯ Additional Improvements to Consider

### **Group 1: Feed Quality & Relevance**

#### **1.1: Automated Feed Health Scoring** â­â­â­
**Problem:** Some feeds are dead, slow, or low-signal  
**Current:** Manual checking  
**Proposal:**
```yaml
feed_health_monitoring:
  enabled: true
  track_metrics:
    - fetch_success_rate (target: >90%)
    - avg_articles_per_day (flag if <2)
    - avg_match_rate (flag if <5%)
    - response_time (flag if >10s)
  
  auto_disable:
    - 3 consecutive failures = disable feed
    - 7 days no matches = move to "Scraping Candidates"
    - Response time >15s = throttle to hourly
  
  dashboard: Generate weekly report of feed health
```

**Implementation:** 2-3 hours  
**Benefit:** Self-healing feed list, removes dead sources automatically

---

#### **1.2: Source Reputation System** â­â­
**Problem:** All sources weighted equally; some blogs are noise  
**Current:** Tiered by domain (4 tiers)  
**Proposal:**
```python
# Track per-source metrics
source_reputation = {
  'domain': {
    'accuracy_score': 0.92,      # How often matched our keywords
    'timeliness_score': 0.88,    # Speed to break news
    'uniqueness_score': 0.75,    # Original content vs aggregation
    'authority_score': 0.95,     # NIST/CISA/etc
    'overall_rank': 0-100
  }
}

# Boost high-reputation sources in sorting
story_score = mention_count Ã— recency Ã— reputation_multiplier
```

**Implementation:** 4-5 hours  
**Benefit:** Learn which sources are most valuable; auto-promote them

---

### **Group 2: Content Filtering & Deduplication**

#### **2.1: Semantic Deduplication** â­â­â­â­
**Problem:** Fuzzy matching misses semantically identical stories  
**Current:** String-based fuzzy matching (0.92 threshold for CVEs)  
**Example:** "Apache RCE" vs "Remote Code Execution Apache" match only at 60%  
**Proposal:**
```python
# Use embedding-based similarity (Sentence-BERT)
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')

def semantic_similarity(title1, title2):
  emb1 = model.encode(title1)
  emb2 = model.encode(title2)
  return cosine_similarity(emb1, emb2)  # 0-1 score

# Use for deduplication
if semantic_similarity(title1, title2) > 0.85:
  merge_entries()
```

**Implementation:** 3-4 hours + 15MB dependency  
**Benefit:** **99% accurate** deduplication vs 85% with fuzzy matching  
**Cost:** +0.5ms per article comparison

---

#### **2.2: Keyword Expansion & Synonyms** â­â­â­
**Problem:** Keywords don't catch related terms  
**Current:** Literal word matching ("CVE-" only matches CVE prefix)  
**Example:** Miss "vulnerability", "exploit", "weakness", "flaw" unless explicitly listed  
**Proposal:**
```yaml
keywords:
  # Current
  - "CVE-"
  - "critical vulnerability"
  
  # Enhanced with synonyms
  vulnerability_related:
    terms: ["CVE-", "vulnerability", "exploit", "weakness", "flaw", "bug"]
    synonyms:
      vulnerability: [exploit, flaw, weakness, 0-day, zero-day]
      breach: [data leak, incident, intrusion, compromise]
      ransomware: [encryption attack, extortion, WannaCry-like]

# Enable synonym expansion
filter_config:
  expand_keywords: true
  synonym_depth: 2  # How deep to search synonyms
```

**Implementation:** 1-2 hours  
**Benefit:** Catch 40-50% more relevant articles without adding keywords manually

---

#### **2.3: Negative Keywords (Exclusion)** â­â­â­
**Problem:** Get spam or off-topic articles  
**Current:** No exclusion system  
**Example:** PR agency news, marketing, press releases mistaken for real news  
**Proposal:**
```yaml
filters:
  negative_keywords:
    - "press release"
    - "announces partnership"
    - "acquisition"
    - "sponsored"
    - "webinar"
    - "case study"
    - "white paper"
    - "advertisement"
  
  negative_domains:
    - "*.medium.com"          # Too much noise
    - "reddit.com"            # Self-promotion
    - "*.linkedin.com"        # Sales spam
```

**Implementation:** 30 minutes  
**Benefit:** Filter out 30% of low-signal content immediately

---

### **Group 3: Article Content & Display**

#### **3.1: Smarter Article Summaries** â­â­â­
**Problem:** AI summaries sometimes miss key details  
**Current:** Gemini summarizes full article text (2,000 chars)  
**Proposal:**
```python
def extract_key_sections(article_html):
  # Parse HTML structure
  sections = {
    'title': extract_title(),
    'lead_paragraph': extract_first_meaningful_paragraph(),  # Usually has meat
    'bullet_points': extract_lists(),
    'code_blocks': extract_code(),
    'citations': extract_citations()
  }
  
  # Feed to Gemini with sections marked
  prompt = f"""
  Summarize this article, prioritizing:
  1. Lead: {sections['lead']}
  2. Bullets: {sections['bullets']}
  3. Citations: {sections['citations']}
  
  Focus on: WHAT, WHY, ACTION ITEMS
  """
```

**Implementation:** 2-3 hours  
**Benefit:** Better summaries, fewer LLM tokens, cleaner output

---

#### **3.2: CVSS/Severity Scoring** â­â­â­â­
**Problem:** Can't distinguish critical vs minor vulnerabilities at a glance  
**Current:** All CVEs treated equally  
**Proposal:**
```python
def extract_cvss_score(article):
  patterns = [
    r'CVSS[:\s]+([0-9.]+)',
    r'severity[:\s]+(critical|high|medium|low)',
    r'cvss[_\s]?score[:\s]+([0-9.]+)'
  ]
  # Look in title, summary, body
  # Extract CVSS numerical score and severity label

# Create severity badge in output
severity = {
  'score': 9.8,
  'label': 'CRITICAL',
  'color': 'red'
}

# Sort by CVSS in highlights
highlights.sort(key=lambda x: x.get('cvss_score', 0), reverse=True)
```

**Implementation:** 2 hours  
**Benefit:** Immediate visual understanding of threat severity

---

#### **3.3: Attribution & Source Confidence** â­â­
**Problem:** Readers don't know if story is confirmed or rumor  
**Current:** All sources trusted equally  
**Proposal:**
```markdown
# Output Example:
CVE-2026-24407: Critical RCE in iccDEV

ğŸ”´ **CRITICAL** (CVSS 9.8)
ğŸ“Š **Confidence:** High (8 authoritative sources)
- âœ… NIST Official Record (tier 1)
- âœ… NVD Database (tier 1)
- âœ… Vendor Advisory (tier 2)
- âš ï¸ Blog Coverage (tier 3)

**Status:** 
- ğŸŸ¢ Confirmed: Vendor released patch
- ğŸŸ¡ Active: Exploitation observed in the wild
- ğŸ”´ Critical: Widespread scanning detected
```

**Implementation:** 3-4 hours  
**Benefit:** Readers understand authority and confidence level

---

### **Group 4: Performance & Cost Optimization**

#### **4.1: Incremental Feed Updates** â­â­â­
**Problem:** Fetches all 7 days worth every run (wasteful)  
**Current:** Full fetch every Monday  
**Proposal:**
```yaml
feed_caching:
  enabled: true
  strategy: "incremental"
  
  track_per_feed:
    last_fetch_time: timestamp
    last_article_date: ISO8601
    articles_cached: N
  
  on_new_run:
    foreach feed:
      if feed has new articles since last_fetch:
        fetch_new_articles()
        merge_with_cache()
      else:
        skip_feed()
    
    # Only process fresh articles
    process_new_batch()

  benefits:
    - 50-70% fewer HTTP requests
    - 30-40% faster processing
    - Lower bandwidth usage
```

**Implementation:** 3-4 hours  
**Benefit:** Speed up aggregation, reduce server load

---

#### **4.2: Batch Gemini Processing** â­â­
**Problem:** Rate limits on API calls; each article calls Gemini individually  
**Current:** 10 articles = 10 API calls  
**Proposal:**
```python
# Batch processing
batch_size = 5
batches = chunks(top_articles, batch_size)

for batch in batches:
  # Use Gemini's batch API (if available)
  # Or: Group 5 summarization tasks into 1 call
  prompt = f"""
  Summarize these 5 articles:
  
  Article 1: {article1['text']}
  Article 2: {article2['text']}
  ...
  
  Provide 5 one-sentence summaries.
  """
  
  results = gemini_api.batch_generate([prompt])
  # Parse and distribute summaries
```

**Implementation:** 1-2 hours  
**Benefit:** Token cost -40%, faster processing

---

#### **4.3: Feed Compression & Archive** â­â­
**Problem:** Weekly posts pile up; search becomes slow  
**Current:** All posts in `site/content/newsfeed/`  
**Proposal:**
```
site/content/newsfeed/
â”œâ”€â”€ 2026-latest/
â”‚   â”œâ”€â”€ 2026-02-22-weekly-scan.md
â”‚   â”œâ”€â”€ 2026-02-15-weekly-scan.md
â”‚   â””â”€â”€ ... (last 8 weeks = 2 months)
â””â”€â”€ archive/
    â””â”€â”€ 2026-01/
        â”œâ”€â”€ 2026-01-29-weekly-scan.md
        â””â”€â”€ 2026-01-22-weekly-scan.md

# In Astro routing:
# /newsfeed/latest â†’ shows last 8 weeks
# /newsfeed/archive/2026-01 â†’ older posts
```

**Implementation:** 2 hours  
**Benefit:** Faster page loads, cleaner directory structure

---

### **Group 5: User Experience**

#### **5.1: Interactive Feed Filter UI** â­â­â­
**Problem:** Categories buried; hard to focus on one area  
**Current:** All articles in one long page  
**Proposal:**
```html
<!-- On newsfeed page -->
<div class="feed-filters">
  <button data-category="all">All News</button>
  <button data-category="cve">ğŸ”´ Vulnerabilities</button>
  <button data-category="ai">ğŸ¤– AI/ML</button>
  <button data-category="cloud">â˜ï¸ Cloud</button>
  <button data-category="regulatory">ğŸ“‹ Compliance</button>
  
  <input type="date" placeholder="From" />
  <input type="date" placeholder="To" />
  <button>Filter</button>
</div>

<!-- Astro component handles filtering client-side or generates variants -->
```

**Implementation:** 4-5 hours (Astro component + styling)  
**Benefit:** Readers can explore by interest

---

#### **5.2: Email Digest Option** â­â­
**Problem:** Some readers prefer email over checking site  
**Current:** No email distribution  
**Proposal:**
```yaml
# config.yaml
email_digest:
  enabled: true
  schedule: "monday-8am"
  format: "html"
  recipients:
    - subscribe@example.com
    - api_key: SendGrid/Mailgun key
  
  template:
    subject: "Weekly Security & AI News: {{ date }}"
    header: "Your automated digest of top stories"
    content:
      - top_5_stories
      - cvss_scorecard
      - category_summary
    footer: "Read full posts at: [link]"
```

**Implementation:** 3-4 hours (requires email service integration)  
**Benefit:** Reach passive readers, increase engagement

---

#### **5.3: Story Timeline/Evolution View** â­â­â­
**Problem:** Readers can't see how a story developed  
**Current:** Only latest snapshot  
**Proposal:**
```markdown
# CVE-2026-24407 | Story Timeline

Jan 22, 08:15 ğŸŸ¡ **NEW:** VulDB registers vulnerability
  > First disclosure by security researcher
  ğŸ‘‰ [VulDB](link)

Jan 22, 14:20 ğŸŸ¡ **CONFIRMED:** Vendor acknowledges via advisory
  > Attack vector details emerge
  ğŸ‘‰ [SecurityWeek](link)

Jan 23, 09:00 ğŸ”´ **EXPLOIT AVAILABLE:** PoC published on GitHub
  > Public exploitation begins
  ğŸ‘‰ [HackerNews](link)

Jan 24, 11:00 ğŸŸ¢ **PATCH RELEASED:** Version 2.3.2 available
  > Immediate update recommended
  ğŸ‘‰ [NVD](link) | [Patch](link)

Jan 24, 14:30 âš ï¸ **ACTIVE SCANNING:** Widespread reconnaissance
  > Attackers scanning for vulnerable systems
  ğŸ‘‰ [Help Net Security](link)
```

**Implementation:** 5-6 hours (requires tracking story evolution)  
**Benefit:** Full context of how threats develop

---

## ğŸ¨ Implementation Priority Matrix

| Feature | Effort | Impact | Priority | Effort | Value/Hour |
|---------|--------|--------|----------|--------|-----------|
| **4.1: Incremental Caching** | 3-4h | ğŸŸ¢ğŸŸ¢ (speed) | P1 | 3.5h | High |
| **2.1: Semantic Dedup** | 3-4h | ğŸŸ¢ğŸŸ¢ğŸŸ¢ (accuracy) | P1 | 3.5h | Very High |
| **3.2: CVSS Scoring** | 2h | ğŸŸ¢ğŸŸ¢ (UX) | P2 | 2h | Very High |
| **1.1: Feed Health** | 2-3h | ğŸŸ¢ (maintenance) | P2 | 2.5h | High |
| **2.2: Keyword Synonyms** | 1-2h | ğŸŸ¢ (recall) | P2 | 1.5h | Very High |
| **2.3: Negative Keywords** | 0.5h | ğŸŸ¢ (precision) | P2 | 0.5h | Very High |
| **3.1: Better Summaries** | 2-3h | ğŸŸ¢ (quality) | P3 | 2.5h | High |
| **5.1: Filter UI** | 4-5h | ğŸŸ¢ (UX) | P3 | 4.5h | Medium |
| **3.3: Source Attribution** | 3-4h | ğŸŸ¢ (trust) | P3 | 3.5h | High |
| **4.2: Batch Processing** | 1-2h | ğŸŸ¡ (cost) | P4 | 1.5h | Medium |
| **4.3: Archive Compression** | 2h | ğŸŸ¡ (perf) | P4 | 2h | Medium |
| **5.2: Email Digest** | 3-4h | ğŸŸ¡ (reach) | P4 | 3.5h | Low |
| **5.3: Story Timeline** | 5-6h | ğŸŸ¢ğŸŸ¢ (insight) | P4 | 5.5h | High |

---

## ğŸ’¡ Recommended Next Steps

### **Quick Wins (3 hours total):**
1. âœ… Add negative keywords (30 min)
2. âœ… Implement keyword synonyms (1.5 hours)
3. âœ… Add CVSS extraction (1 hour)

**Result:** Better filtering + better CVE visibility

### **High Impact (7-8 hours total):**
4. Semantic deduplication with embeddings (3-4 hours)
5. Incremental feed caching (3-4 hours)

**Result:** Faster, more accurate aggregation

### **Future Phase (10-15 hours):**
6. Feed health monitoring
7. Story timeline tracking
8. Feed reputation system

---

## ğŸ”§ My Recommendations (Ranked)

### ğŸ¥‡ **Must Do (Next Sprint):**
- âœ… **2.3: Negative Keywords** - 30 min, huge impact on signal-to-noise
- âœ… **3.2: CVSS Scoring** - 2 hours, makes CVE articles actionable
- âœ… **2.2: Keyword Synonyms** - 1.5 hours, catches 40% more articles

### ğŸ¥ˆ **Should Do (Following Sprint):**
- **2.1: Semantic Deduplication** - 3-4 hours, best accuracy
- **4.1: Incremental Caching** - 3-4 hours, speeds everything up

### ğŸ¥‰ **Nice to Have (Future):**
- **5.1: Filter UI** - Better UX for readers
- **3.3: Source Attribution** - Build trust/confidence
- **5.3: Story Timeline** - Show story evolution

---

## ğŸ“‹ Quick Decision Questions

1. **Semantic dedup:** Worth 15MB dependency + 0.5ms per article?
   - [ ] Yes, accuracy is critical
   - [ ] No, stick with fuzzy matching
   - [ ] Maybe, show me the accuracy improvement first

2. **Email digests:** Should we distribute via email?
   - [ ] Yes, many readers prefer email
   - [ ] No, website is enough
   - [ ] Later phase

3. **Feed health:** Auto-disable dead feeds?
   - [ ] Yes, keep list fresh
   - [ ] No, manually manage
   - [ ] Track but don't auto-disable

4. **CVSS scoring:** Priority?
   - [ ] High (do immediately)
   - [ ] Medium (next sprint)
   - [ ] Low (nice to have)

5. **Story Timeline:** Worth tracking story evolution?
   - [ ] Yes, very valuable
   - [ ] No, too complex
   - [ ] Maybe later

---

**Ready to discuss? Reply with the decisions above, and I'll prioritize implementation accordingly!**
